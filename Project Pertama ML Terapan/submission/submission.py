# -*- coding: utf-8 -*-
"""submission.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OJX89PyZjqVDWzuPIR2Lw8RPHHN2gwCs

## Data Preparation

### Load Data
"""

import pandas as pd

df = pd.read_csv("D:\\UB\\Dicoding\\Datasets\\archive\\AirQuality.csv", delimiter=";")

"""### Cleaning Data"""

# 1. Menghapus kolom yang tidak berguna
df.drop(columns=['Unnamed: 15', 'Unnamed: 16'], inplace=True)

df['Time'].fillna('00:00:00', inplace=True)

# Konversi kolom Date terlebih dahulu
df['Date'] = pd.to_datetime(df['Date'], format='%d/%m/%Y')

# Format ulang Time agar sesuai
df['Time'] = df['Time'].str.replace('.', ':')  # Ganti titik dengan titik dua
df['Time'] = pd.to_datetime(df['Time'], format='%H:%M:%S', errors='coerce').dt.time  # Pastikan hanya waktu

# Gabungkan menjadi datetime dengan format yang jelas
df['Datetime'] = pd.to_datetime(df['Date'].astype(str) + ' ' + df['Time'].astype(str), errors='coerce')

# 3. Mengonversi kolom numerik dan menangani nilai yang salah
# Mengganti koma dengan titik pada kolom yang perlu
columns_to_replace_commas = ['CO(GT)', 'C6H6(GT)', 'T', 'RH', 'AH']
for col in columns_to_replace_commas:
    df[col] = df[col].str.replace(',', '.').astype(float)  # Ganti ',' dengan '.' dan konversi ke float

# Memastikan kolom CO(GT) sepenuhnya menjadi float dan tidak ada kesalahan
df['CO(GT)'] = pd.to_numeric(df['CO(GT)'], errors='coerce')  # Konversi dan ganti yang tidak bisa konversi menjadi NaN

# 4. Menangani nilai yang hilang dengan penghapusan atau imputasi
# Untuk contoh ini, kita akan menghapus baris yang memiliki nilai NaN
df.dropna(inplace=True)

# 5. Memeriksa duplikasi
df.drop_duplicates(inplace=True)

df.drop(columns=['Date', 'Time'], inplace=True)

# (Opsional) Menyimpan DataFrame yang sudah dibersihkan ke file baru
df.to_csv('AirQuality_Cleaned.csv', index=False, sep=';')

df.info()
df.head()

# Menampilkan informasi DataFrame awal
print("DataFrame sebelum normalisasi:")
print(df.describe())

# 1. Normalisasi Menggunakan Min-Max Scaling
from sklearn.preprocessing import MinMaxScaler

# Menginisialisasi scaler
scaler = MinMaxScaler()

# Kolom yang akan dinormalisasi
columns_to_normalize = [
    'CO(GT)', 'PT08.S1(CO)', 'NMHC(GT)', 'C6H6(GT)',
    'PT08.S2(NMHC)', 'NOx(GT)', 'PT08.S3(NOx)',
    'NO2(GT)', 'PT08.S4(NO2)', 'PT08.S5(O3)',
    'T', 'RH', 'AH'
]

# Melakukan normalisasi
df[columns_to_normalize] = scaler.fit_transform(df[columns_to_normalize])

# Menampilkan informasi DataFrame setelah normalisasi
print("\nDataFrame setelah normalisasi:")
print(df.describe())

# (Opsional) Menyimpan DataFrame yang dinormalisasi ke file baru
df.to_csv('AirQuality_Normalized.csv', index=False, sep=';')

"""### Visualisasi Data"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Mengatur gaya seaborn untuk visualisasi
sns.set(style='whitegrid')

# 1. Visualisasi scatter plot untuk memprediksi konsentrasi polutan
# Kita pilih contoh polutan NO2(GT) dan fitur suhu (T)
plt.figure(figsize=(12, 6))

# Scatter plot NO2(GT) vs Temperature (T)
plt.subplot(1, 2, 1)
sns.scatterplot(data=df, x='T', y='NO2(GT)', color='blue')
plt.title('Scatter Plot: NO2 vs Temperature')
plt.xlabel('Temperature (°C)')
plt.ylabel('NO2 Concentration (µg/m³)')

# Scatter plot NO2(GT) vs Relative Humidity (RH)
plt.subplot(1, 2, 2)
sns.scatterplot(data=df, x='RH', y='NO2(GT)', color='red')
plt.title('Scatter Plot: NO2 vs Relative Humidity')
plt.xlabel('Relative Humidity (%)')
plt.ylabel('NO2 Concentration (µg/m³)')

plt.tight_layout()
plt.show()

# 2. Analisis Fitur: Heatmap untuk visualisasi korelasi
plt.figure(figsize=(10, 8))
correlation_matrix = df.corr()

# Heatmap
sns.heatmap(correlation_matrix, annot=True, fmt=".2f", cmap='coolwarm', square=True, cbar_kws={"shrink": .8})
plt.title('Heatmap of Feature Correlations')
plt.show()

"""## Modelling"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# 1. Memisahkan fitur dan target
X = df.drop(columns=['CO(GT)', 'Datetime'])  # Fitur (semua kolom kecuali target dan timestamp)
y = df['CO(GT)']  # Target (konsentrasi CO)

# 2. Membagi dataset menjadi train dan test dengan proporsi 80% untuk training dan 20% untuk testing
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 3. Melatih model Linear Regression
linear_model = LinearRegression()
linear_model.fit(X_train, y_train)

# 4. Melatih model Random Forest Regression
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

# 5. Melakukan prediksi menggunakan kedua model
y_pred_linear = linear_model.predict(X_test)
y_pred_rf = rf_model.predict(X_test)

# 6. Evaluasi Model
def evaluate_model(y_true, y_pred, model_name):
    mae = mean_absolute_error(y_true, y_pred)
    rmse = mean_squared_error(y_true, y_pred) ** 0.5  # Hitung RMSE secara manual
    r2 = r2_score(y_true, y_pred)

    print(f"Evaluation Metrics for {model_name}:")
    print(f"Mean Absolute Error: {mae:.4f}")
    print(f"Root Mean Squared Error: {rmse:.4f}")
    print(f"R² Score: {r2:.4f}\n")

# Evaluasi untuk Linear Regression
evaluate_model(y_test, y_pred_linear, "Linear Regression")

# Evaluasi untuk Random Forest Regression
evaluate_model(y_test, y_pred_rf, "Random Forest Regression")

from sklearn.model_selection import GridSearchCV

# Menginisialisasi model Random Forest
rf_model = RandomForestRegressor(random_state=42)

# Menentukan parameter grid untuk dicari
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
}

# Menggunakan GridSearchCV untuk menemukan hyperparameters terbaik
grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid,
                           cv=3, scoring='neg_mean_squared_error',
                           verbose=2, n_jobs=-1)
grid_search.fit(X_train, y_train)

# Menampilkan parameter terbaik
print("Best parameters:", grid_search.best_params_)

# Memprediksi menggunakan model terbaik
best_rf_model = grid_search.best_estimator_
y_pred_best_rf = best_rf_model.predict(X_test)

# Evaluasi model terbaik
evaluate_model(y_test, y_pred_best_rf, "Optimized Random Forest Regression")

# Contoh profil input untuk prediksi
# Format: [ PT08.S1(CO), NMHC(GT), C6H6(GT), PT08.S2(NMHC), NOx(GT), PT08.S3(NOx), NO2(GT), PT08.S4(NO2), PT08.S5(O3), T, RH, AH ]
input_profiles = [
    [0.5, 0.02, 0.75, 0.45, 0.2, 0.35, 0.5, 0.54, 0.45, 0.85, 0.8, 0.95],  # Profil 1
    [0.7, 0.03, 0.78, 0.5, 0.25, 0.4, 0.55, 0.6, 0.5, 0.75, 0.82, 0.93],  # Profil 2
    [0.6, 0.04, 0.80, 0.48, 0.3, 0.42, 0.57, 0.55, 0.49, 0.78, 0.79, 0.91],  # Profil 3
    [0.55, 0.01, 0.70, 0.46, 0.15, 0.38, 0.52, 0.53, 0.52, 0.88, 0.76, 0.97],  # Profil 4
    [0.8, 0.07, 0.85, 0.52, 0.4, 0.48, 0.6, 0.62, 0.55, 0.72, 0.77, 0.89],  # Profil 5
    [0.9, 0.05, 0.88, 0.55, 0.35, 0.45, 0.65, 0.65, 0.57, 0.89, 0.74, 0.8],  # Profil 6
    [0.4, 0.02, 0.60, 0.44, 0.1, 0.33, 0.45, 0.44, 0.4, 0.7, 0.6, 0.92],  # Profil 7
    [0.75, 0.04, 0.76, 0.5, 0.3, 0.4, 0.53, 0.55, 0.51, 0.76, 0.78, 0.94],  # Profil 8
    [0.65, 0.05, 0.79, 0.49, 0.2, 0.39, 0.58, 0.58, 0.54, 0.83, 0.75, 0.91],  # Profil 9
    [0.85, 0.03, 0.82, 0.53, 0.45, 0.47, 0.62, 0.6, 0.56, 0.82, 0.79, 0.88]   # Profil 10
]

# Mengonversi input ke DataFrame untuk prediksi
input_df = pd.DataFrame(input_profiles, columns=[
    'PT08.S1(CO)', 'NMHC(GT)', 'C6H6(GT)', 'PT08.S2(NMHC)',
    'NOx(GT)', 'PT08.S3(NOx)', 'NO2(GT)', 'PT08.S4(NO2)',
    'PT08.S5(O3)', 'T', 'RH', 'AH'
])

# Melakukan prediksi menggunakan model yang dilatih
predictions = rf_model.predict(input_df)

# Menampilkan hasil prediksi
for i, prediction in enumerate(predictions):
    print(f"Prediksi untuk Profil {i + 1}: {prediction:.4f}")