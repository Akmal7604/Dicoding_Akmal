{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33bd9fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "baba4121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Membuat daftar peta\n",
    "peta = [\n",
    "    ['SFFF','FHFH','FFFH','HFFG'],\n",
    "    ['SFFF','FFHF','HFFF','HFFG'],\n",
    "    ['SHFF','FHFH','FFFH','HHFG'],\n",
    "    ['SFFF','HHFF','FFFF','HFFG'],\n",
    "    ['SFFH','FFFH','HFFH','HHFG']\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afc562fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memuat lingkungan\n",
    "env = gym.make(\"FrozenLake-v0\",is_slippery=False, desc=peta[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eee9cb82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Banyak State  : 16\n",
      "Banyak Action : 4\n"
     ]
    }
   ],
   "source": [
    "n_observations = env.observation_space.n\n",
    "n_actions      = env.action_space.n\n",
    " \n",
    "print('Banyak State  : ' + str(n_observations))\n",
    "print('Banyak Action : ' + str(n_actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "353c8753",
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTION = [\"KIRI\",\"BAWAH\",\"KANAN\",\"ATAS\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95276f93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47188b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New State : 1\n",
      "Reward    : 0.0\n",
      "Done      : False\n",
      "  (Right)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "# Langkah 1 (ke Kanan)\n",
    "new_state, reward, done, info = env.step(2)\n",
    " \n",
    "# Menampilkan informasi\n",
    "print(f\"New State : {new_state}\")\n",
    "print(f\"Reward    : {reward}\")\n",
    "print(f\"Done      : {done}\")\n",
    " \n",
    "# Menampilkan visualisasi lingkungan\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2decbb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New State : 2\n",
      "Reward    : 0.0\n",
      "Done      : False\n",
      "  (Right)\n",
      "SF\u001b[41mF\u001b[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "# Langkah 2 (ke Kanan)\n",
    "new_state, reward, done, info = env.step(2)\n",
    " \n",
    "# Menampilkan informasi\n",
    "print(f\"New State : {new_state}\")\n",
    "print(f\"Reward    : {reward}\")\n",
    "print(f\"Done      : {done}\")\n",
    " \n",
    "# Menampilkan visualisasi lingkungan\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4bad160e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New State : 6\n",
      "Reward    : 0.0\n",
      "Done      : False\n",
      "  (Down)\n",
      "SFFF\n",
      "FH\u001b[41mF\u001b[0mH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "# Langkah 3 (ke Bawah)\n",
    "new_state, reward, done, info = env.step(1)\n",
    " \n",
    "# Menampilkan informasi\n",
    "print('New State : {}'.format(new_state))\n",
    "print('Reward    : {}'.format(reward))\n",
    "print('Done      : {}'.format(done))\n",
    " \n",
    "# Menampilkan visualisasi lingkungan\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a6d3ce92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New State : 10\n",
      "Reward    : 0.0\n",
      "Done      : False\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FF\u001b[41mF\u001b[0mH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "# Langkah 4 (ke Bawah)\n",
    "new_state, reward, done, info = env.step(1)\n",
    " \n",
    "# Menampilkan informasi\n",
    "print('New State : {}'.format(new_state))\n",
    "print('Reward    : {}'.format(reward))\n",
    "print('Done      : {}'.format(done))\n",
    " \n",
    "# Menampilkan visualisasi lingkungan\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b5a3aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New State : 14\n",
      "Reward    : 0.0\n",
      "Done      : False\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n"
     ]
    }
   ],
   "source": [
    "# Langkah 5 (ke Bawah)\n",
    "new_state, reward, done, info = env.step(1)\n",
    " \n",
    "# Menampilkan informasi\n",
    "print(f\"New State : {new_state}\")\n",
    "print(f\"Reward    : {reward}\")\n",
    "print(f\"Done      : {done}\")\n",
    " \n",
    "# Menampilkan visualisasi lingkungan\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "75413574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New State : 15\n",
      "Reward    : 1.0\n",
      "Done      : True\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Langkah 6 (ke Kanan)\n",
    "new_state, reward, done, info = env.step(2)\n",
    " \n",
    "# Menampilkan informasi\n",
    "print(f\"New State : {new_state}\")\n",
    "print(f\"Reward    : {reward}\")\n",
    "print(f\"Done      : {done}\")\n",
    " \n",
    "# Menampilkan visualisasi lingkungan\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f054e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(env, n_episodes=10000, max_iter_episode=100, exploration_proba=1, exploration_decreasing_decay=0.001, min_exploration_proba=0.01, gamma=0.99, lr=0.1):\n",
    "    # Inisialisasi Q-table dengan ukuran berdasarkan jumlah state dan aksi\n",
    "    Q_table = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "    \n",
    "    # List untuk menyimpan reward dari setiap episode\n",
    "    rewards_per_episode = []\n",
    " \n",
    "    # Loop melalui setiap episode\n",
    "    for episode in range(n_episodes):\n",
    "        # Reset lingkungan untuk memulai episode baru dan mendapatkan state awal\n",
    "        state = env.reset()\n",
    "        \n",
    "        # Inisialisasi total reward episode menjadi 0\n",
    "        episode_reward = 0\n",
    " \n",
    "        # Loop melalui setiap iterasi dalam episode\n",
    "        for _ in range(max_iter_episode):\n",
    "            # Pilih tindakan berdasarkan probabilitas eksplorasi atau menggunakan kebijakan Q\n",
    "            if np.random.uniform(0, 1) < exploration_proba:\n",
    "                action = env.action_space.sample()  # Aksi acak (eksplorasi)\n",
    "            else:\n",
    "                action = np.argmax(Q_table[state, :])  # Aksi terbaik berdasarkan Q-table (eksploitasi)\n",
    " \n",
    "            # Ambil langkah berdasarkan tindakan yang dipilih\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    " \n",
    "            # Update Q-value berdasarkan reward yang diterima dan perkiraan nilai Q di state berikutnya\n",
    "            Q_table[state, action] = (1 - lr) * Q_table[state, action] + lr * (reward + gamma * np.max(Q_table[next_state, :]))\n",
    " \n",
    "            # Tambahkan reward dari langkah ini ke total reward episode\n",
    "            episode_reward += reward\n",
    "            state = next_state  # Pindah ke state berikutnya\n",
    " \n",
    "            # Hentikan episode jika mencapai terminal state\n",
    "            if done:\n",
    "                break\n",
    " \n",
    "        # Kurangi probabilitas eksplorasi seiring berjalannya waktu\n",
    "        exploration_proba = max(min_exploration_proba, np.exp(-exploration_decreasing_decay * episode))\n",
    "        \n",
    "        # Simpan total reward episode ke dalam list\n",
    "        rewards_per_episode.append(episode_reward)\n",
    " \n",
    "    # Cetak rata-rata reward per 1000 episode\n",
    "    print(\"Rata-Rata Reward per 1000 Episode\")\n",
    "    for i in range(10):\n",
    "        print((i + 1) * 1000, \" : Rata-Rata Reward: \", np.mean(rewards_per_episode[1000 * i:1000 * (i + 1)]))\n",
    " \n",
    "    # Kembalikan Q-table yang telah dilatih\n",
    "    return Q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c6cae3b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peta : \n",
      "['SFFF', 'FHFH', 'FFFH', 'HFFG']\n",
      "Rata-Rata Reward per 1000 Episode\n",
      "1000  : Rata-Rata Reward:  0.282\n",
      "2000  : Rata-Rata Reward:  0.746\n",
      "3000  : Rata-Rata Reward:  0.911\n",
      "4000  : Rata-Rata Reward:  0.974\n",
      "5000  : Rata-Rata Reward:  0.992\n",
      "6000  : Rata-Rata Reward:  0.99\n",
      "7000  : Rata-Rata Reward:  0.985\n",
      "8000  : Rata-Rata Reward:  0.986\n",
      "9000  : Rata-Rata Reward:  0.988\n",
      "10000  : Rata-Rata Reward:  0.99\n",
      "\n",
      "Peta : \n",
      "['SFFF', 'FFHF', 'HFFF', 'HFFG']\n",
      "Rata-Rata Reward per 1000 Episode\n",
      "1000  : Rata-Rata Reward:  0.347\n",
      "2000  : Rata-Rata Reward:  0.815\n",
      "3000  : Rata-Rata Reward:  0.948\n",
      "4000  : Rata-Rata Reward:  0.98\n",
      "5000  : Rata-Rata Reward:  0.991\n",
      "6000  : Rata-Rata Reward:  0.991\n",
      "7000  : Rata-Rata Reward:  0.989\n",
      "8000  : Rata-Rata Reward:  0.992\n",
      "9000  : Rata-Rata Reward:  0.996\n",
      "10000  : Rata-Rata Reward:  0.991\n",
      "\n",
      "Peta : \n",
      "['SHFF', 'FHFH', 'FFFH', 'HHFG']\n",
      "Rata-Rata Reward per 1000 Episode\n",
      "1000  : Rata-Rata Reward:  0.136\n",
      "2000  : Rata-Rata Reward:  0.598\n",
      "3000  : Rata-Rata Reward:  0.868\n",
      "4000  : Rata-Rata Reward:  0.939\n",
      "5000  : Rata-Rata Reward:  0.976\n",
      "6000  : Rata-Rata Reward:  0.983\n",
      "7000  : Rata-Rata Reward:  0.978\n",
      "8000  : Rata-Rata Reward:  0.982\n",
      "9000  : Rata-Rata Reward:  0.989\n",
      "10000  : Rata-Rata Reward:  0.977\n",
      "\n",
      "Peta : \n",
      "['SFFF', 'HHFF', 'FFFF', 'HFFG']\n",
      "Rata-Rata Reward per 1000 Episode\n",
      "1000  : Rata-Rata Reward:  0.418\n",
      "2000  : Rata-Rata Reward:  0.852\n",
      "3000  : Rata-Rata Reward:  0.967\n",
      "4000  : Rata-Rata Reward:  0.985\n",
      "5000  : Rata-Rata Reward:  0.999\n",
      "6000  : Rata-Rata Reward:  0.994\n",
      "7000  : Rata-Rata Reward:  0.998\n",
      "8000  : Rata-Rata Reward:  0.992\n",
      "9000  : Rata-Rata Reward:  0.995\n",
      "10000  : Rata-Rata Reward:  0.997\n",
      "\n",
      "Peta : \n",
      "['SFFH', 'FFFH', 'HFFH', 'HHFG']\n",
      "Rata-Rata Reward per 1000 Episode\n",
      "1000  : Rata-Rata Reward:  0.311\n",
      "2000  : Rata-Rata Reward:  0.774\n",
      "3000  : Rata-Rata Reward:  0.935\n",
      "4000  : Rata-Rata Reward:  0.974\n",
      "5000  : Rata-Rata Reward:  0.993\n",
      "6000  : Rata-Rata Reward:  0.993\n",
      "7000  : Rata-Rata Reward:  0.991\n",
      "8000  : Rata-Rata Reward:  0.994\n",
      "9000  : Rata-Rata Reward:  0.991\n",
      "10000  : Rata-Rata Reward:  0.992\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Q_table_all = []  # List kosong untuk menyimpan semua tabel Q dari setiap lingkungan\n",
    " \n",
    "for peta_env in peta:  # Loop melalui setiap peta dalam daftar peta\n",
    "    # Load Environment untuk setiap peta dengan konfigurasi tertentu\n",
    "    env = gym.make(\"FrozenLake-v0\", is_slippery=False, desc=peta_env)\n",
    "    env.reset()  # Reset lingkungan ke keadaan awal\n",
    " \n",
    "    print('Peta : ')\n",
    "    print(peta_env)  # Cetak peta yang sedang digunakan\n",
    " \n",
    "    # Melatih Agent pada lingkungan saat ini\n",
    "    Q_table = train_agent(env)  # Panggil fungsi train_agent untuk melatih agen di lingkungan saat ini\n",
    " \n",
    "    # Menyimpan Q_table untuk lingkungan saat ini ke dalam list\n",
    "    Q_table_all.append(Q_table)  # Tambahkan Q_table ke dalam list Q_table_all\n",
    " \n",
    "    print()  # Cetak baris kosong untuk pemisah antara lingkungan yang berbeda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a9b06154",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_peta = 0 # silakan pilih peta\n",
    " \n",
    "env = gym.make(\"FrozenLake-v0\",is_slippery=False, desc=peta[index_peta])\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "27b822a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Langkah ke  : 1\n",
      "Best Action : BAWAH\n",
      "New State   : 4\n",
      "Reward      : 0.0\n",
      "Done        : False\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "--------------------------------------\n",
      "Langkah ke  : 2\n",
      "Best Action : BAWAH\n",
      "New State   : 8\n",
      "Reward      : 0.0\n",
      "Done        : False\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "\n",
      "--------------------------------------\n",
      "Langkah ke  : 3\n",
      "Best Action : KANAN\n",
      "New State   : 9\n",
      "Reward      : 0.0\n",
      "Done        : False\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "\n",
      "--------------------------------------\n",
      "Langkah ke  : 4\n",
      "Best Action : BAWAH\n",
      "New State   : 13\n",
      "Reward      : 0.0\n",
      "Done        : False\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "\n",
      "--------------------------------------\n",
      "Langkah ke  : 5\n",
      "Best Action : KANAN\n",
      "New State   : 14\n",
      "Reward      : 0.0\n",
      "Done        : False\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "\n",
      "--------------------------------------\n",
      "Langkah ke  : 6\n",
      "Best Action : KANAN\n",
      "New State   : 15\n",
      "Reward      : 1.0\n",
      "Done        : True\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for langkah in range(1, 7):\n",
    "    if langkah == 1:\n",
    "        best_action = np.argmax(Q_table_all[index_peta][0])\n",
    "    else:\n",
    "        best_action = np.argmax(Q_table_all[index_peta][current_state])\n",
    " \n",
    "    new_state, reward, done, info = env.step(best_action)\n",
    " \n",
    "    # Cetak informasi langkah\n",
    "    print('--------------------------------------')\n",
    "    print('Langkah ke  :', langkah)\n",
    "    print('Best Action :', ACTION[best_action])\n",
    "    print('New State   :', new_state)\n",
    "    print('Reward      :', reward)\n",
    "    print('Done        :', done)\n",
    " \n",
    "    # Tampilkan visualisasi lingkungan\n",
    "    env.render()\n",
    "    current_state = new_state  # Perbarui state saat ini\n",
    " \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "adcf0f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peta   : ['SFFF', 'FHFH', 'FFFH', 'HFFG']\n",
      "Status : Agent dapat menyelesaikan peta ini\n",
      "\n",
      "Peta   : ['SFFF', 'FFHF', 'HFFF', 'HFFG']\n",
      "Status : Agent dapat menyelesaikan peta ini\n",
      "\n",
      "Peta   : ['SHFF', 'FHFH', 'FFFH', 'HHFG']\n",
      "Status : Agent dapat menyelesaikan peta ini\n",
      "\n",
      "Peta   : ['SFFF', 'HHFF', 'FFFF', 'HFFG']\n",
      "Status : Agent dapat menyelesaikan peta ini\n",
      "\n",
      "Peta   : ['SFFH', 'FFFH', 'HFFH', 'HHFG']\n",
      "Status : Agent dapat menyelesaikan peta ini\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for peta_env in peta:  # Iterasi melalui setiap peta dalam daftar peta\n",
    "    print(\"Peta   :\", peta_env)  # Cetak deskripsi peta yang sedang diperiksa\n",
    " \n",
    "    env = gym.make(\"FrozenLake-v0\", is_slippery=False, desc=peta_env)  # Buat lingkungan permainan Frozen Lake\n",
    "    env.reset()  # Atur ulang lingkungan ke keadaan awal\n",
    " \n",
    "    # Iterasi untuk agen melakukan langkah-langkah dalam lingkungan\n",
    "    for langkah in range(1, 7):\n",
    "        if langkah == 1:\n",
    "            best_action = np.argmax(Q_table_all[index_peta][0])  # Ambil tindakan terbaik untuk langkah pertama\n",
    "        else:\n",
    "            best_action = np.argmax(Q_table_all[index_peta][current_state])  # Ambil tindakan terbaik berdasarkan state saat ini\n",
    " \n",
    "        new_state, reward, done, info = env.step(best_action)  # Lakukan langkah terbaik dalam lingkungan\n",
    " \n",
    "        current_state = new_state  # Perbarui state saat ini\n",
    " \n",
    "    # Periksa apakah agen berhasil menyelesaikan permainan\n",
    "    if done:\n",
    "        print(\"Status : Agent dapat menyelesaikan peta ini\")  # Cetak pesan jika agen berhasil menyelesaikan permainan\n",
    "    else:\n",
    "        print(\"Status : Agent tidak dapat menyelesaikan peta ini\")  # Cetak pesan jika agen gagal menyelesaikan permainan\n",
    " \n",
    "    print()  # Cetak baris kosong sebagai pemisah antara hasil dari setiap peta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d5701e45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabel Q untuk lingkungan Frozen Lake telah berhasil disimpan dalam file 'Q_table_Frozen_Lake.model'. Proses pelatihan agen dan penyimpanan tabel Q ini memungkinkan untuk digunakan kembali dalam pengujian atau aplikasi selanjutnya tanpa perlu melakukan pelatihan ulang.\n"
     ]
    }
   ],
   "source": [
    "# Simpan tabel Q yang telah dilatih menggunakan modul pickle\n",
    "pickle.dump(Q_table_all, open('Q_table_Frozen_Lake.model', 'wb'))\n",
    " \n",
    "# Penjelasan penutup\n",
    "print(\"Tabel Q untuk lingkungan Frozen Lake telah berhasil disimpan dalam file 'Q_table_Frozen_Lake.model'. Proses pelatihan agen dan penyimpanan tabel Q ini memungkinkan untuk digunakan kembali dalam pengujian atau aplikasi selanjutnya tanpa perlu melakukan pelatihan ulang.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
